{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0fbd607-e38e-4473-aa37-c81370a81e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import optuna\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e65a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESET Pytorch ###\n",
    "def reset_pytorch(seed=42):\n",
    "    \"\"\"Function to ensure that results from PyTorch models\n",
    "    are consistent and reproducible across different runs (CPU only)\"\"\"\n",
    "    \n",
    "    # 1. Set PYTHONHASHSEED environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # 2. Set python built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 3. Set numpy pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 4. Set torch pseudo-random generator at a fixed value\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # 5. Ensure deterministic operations in PyTorch \n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic results\n",
    "    torch.backends.cudnn.benchmark = False  # Disables cudnn optimizations for reproducibility\n",
    "    torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a0778-b22a-4b64-afd9-3fa33f64b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mamba_chunk_scan_combined(x, dt_expanded, A_expanded, B_expanded, C_expanded, z_expanded, initial_states, n_heads):\n",
    "    x_expanded = x.unsqueeze(1).expand(-1, n_heads, x.size(-2), -1)\n",
    "    \n",
    "    initial_states_expanded = initial_states if initial_states.shape[0] == x.shape[0] else repeat(initial_states, 'b h d -> b h d', b=x.shape[0])\n",
    "    \n",
    "    x_sum = x_expanded.sum(dim=2, keepdim=True)\n",
    "    initial_states_expanded = initial_states_expanded.unsqueeze(2)\n",
    "    evolved_states = initial_states_expanded + (x_sum * A_expanded) * dt_expanded\n",
    "    \n",
    "    evolved_states = evolved_states + B_expanded + C_expanded * z_expanded\n",
    "    return evolved_states\n",
    "\n",
    "\n",
    "def causal_conv1d(x, weight, bias=None):\n",
    "    padding = weight.size(2) - 1\n",
    "    x = F.pad(x, (padding, 0), mode='constant', value=0)\n",
    "    return F.conv1d(x, weight, bias=bias)\n",
    "\n",
    "def silu(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n",
    "        return self.weight * x / norm \n",
    "        \n",
    "\n",
    "class Mamba2Simple(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, d_state=128, d_inner=128, n_heads=14, bias=True):\n",
    "        super(Mamba2Simple, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, d_model)  # First fully connected layer\n",
    "        self.conv1d = nn.Conv1d(d_model, d_inner, kernel_size=3, padding=1)  # 1D Convolution layer\n",
    "        self.norm = RMSNorm(d_inner, eps=1e-5)  # RMS normalization\n",
    "        self.fc2 = nn.Linear(d_inner, 1)  # Final fully connected layer for binary classification\n",
    "\n",
    "        self.dt_bias = nn.Parameter(torch.randn(n_heads))  \n",
    "        self.A_log = nn.Parameter(torch.randn(n_heads))  \n",
    "        self.D = nn.Parameter(torch.ones(n_heads))\n",
    "        self.n_heads = n_heads  \n",
    "        self.d_state = d_state  \n",
    "        self.d_model = d_model  \n",
    "        self.initial_states = nn.Parameter(torch.zeros(self.n_heads, self.d_state))  \n",
    "        \n",
    "        # Input projection\n",
    "        self.in_proj = nn.Linear(self.d_model, 2 * self.d_model + 2 * self.n_heads * self.d_state + self.n_heads, bias=bias)\n",
    "        self.B_proj = nn.Linear(self.d_model, self.n_heads * self.d_state)  # Adjusted projection to match n_heads * d_state\n",
    "        self.C_proj = nn.Linear(self.d_model, self.n_heads * self.d_state)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        batch, seqlen, dim = x.shape\n",
    "\n",
    "        # === Step 1: Create mask for non-padding tokens ===\n",
    "        mask = (x != 666).all(dim=-1)  # shape: (batch, seqlen)\n",
    "        x = x * mask.unsqueeze(-1)  # zero out padded inputs\n",
    "\n",
    "        # === Step 2: First FC layer with activation ===\n",
    "        x = self.fc1(x)\n",
    "        x = silu(x)\n",
    "\n",
    "        # === Step 3: In-projection ===\n",
    "        zxbcdt = self.in_proj(x)\n",
    "        z, x_proj, B, C, dt = torch.split(\n",
    "            zxbcdt,\n",
    "            [self.d_model, self.d_model, self.n_heads * self.d_state, self.n_heads * self.d_state, self.n_heads],\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        # === Step 4: Process dt ===\n",
    "        dt = dt.mean(dim=-1)\n",
    "        dt = dt.mean(dim=0)\n",
    "        dt = F.softplus(dt + self.dt_bias)\n",
    "        dt_expanded = dt.reshape(1, self.n_heads, 1)\n",
    "\n",
    "        # === Step 5: Convolution ===\n",
    "        x_proj = x_proj.transpose(1, 2)\n",
    "        x_proj = causal_conv1d(x_proj, self.conv1d.weight, self.conv1d.bias)\n",
    "\n",
    "        # Mask out invalid positions before max pooling\n",
    "        mask_conv = mask.unsqueeze(1).expand_as(x_proj)  # (B, d_inner, L)\n",
    "        x_proj[~mask_conv] = float('-inf')  # prevent max from seeing padded values\n",
    "        x_conv = x_proj.max(dim=2).values  # (B, d_inner)\n",
    "\n",
    "        # === Step 6: Mamba ops ===\n",
    "        A = torch.exp(self.A_log)\n",
    "        A_expanded = A.view(1, self.n_heads, 1)\n",
    "\n",
    "        B_proj = self.B_proj(x_conv).view(batch, self.n_heads, self.d_state)\n",
    "        C_proj = self.C_proj(x_conv).view(batch, self.n_heads, self.d_state)\n",
    "        z_proj = z.view(batch, self.n_heads, self.d_model)\n",
    "\n",
    "        B_expanded = B_proj.unsqueeze(2).expand(batch, self.n_heads, self.n_heads, self.d_state)\n",
    "        C_expanded = C_proj.unsqueeze(2).expand(batch, self.n_heads, self.n_heads, self.d_state)\n",
    "        z_expanded = z_proj.unsqueeze(2).expand(batch, self.n_heads, self.n_heads, self.d_model)\n",
    "\n",
    "        states_expanded = repeat(self.initial_states, 'h d -> b h d', b=batch)\n",
    "\n",
    "        y = mamba_chunk_scan_combined(\n",
    "            rearrange(x_conv, \"b d -> b d 1\"),\n",
    "            dt_expanded=dt_expanded,\n",
    "            A_expanded=A_expanded,\n",
    "            B_expanded=B_expanded,\n",
    "            C_expanded=C_expanded,\n",
    "            z_expanded=z_expanded,\n",
    "            initial_states=states_expanded,\n",
    "            n_heads=self.n_heads\n",
    "        )\n",
    "\n",
    "        # === Step 7: Combine and normalize ===\n",
    "        x_expanded = x_conv.unsqueeze(1).unsqueeze(2)\n",
    "        x_expanded_for_sum = x_expanded.expand(-1, self.n_heads, self.n_heads, -1)\n",
    "        y = y.squeeze(-1)\n",
    "        x_combined = x_expanded_for_sum + y\n",
    "\n",
    "        y_norm = self.norm(x_combined)\n",
    "        y_pooled = y_norm.max(dim=1).values\n",
    "        y_fc = self.fc2(y_pooled).max(dim=1).values\n",
    "\n",
    "        out = torch.sigmoid(y_fc.squeeze(-1))\n",
    "        return out.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa6c9de-511e-4828-9de1-b8d3985ab63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, mindelta, restore_best_weights=True, mode=\"min\"):\n",
    "        self.patience = patience\n",
    "        self.mindelta = mindelta  \n",
    "        self.restore_best_weights_flag = restore_best_weights\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf if mode == \"min\" else -np.inf\n",
    "        self.best_model_state = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.mode == \"min\":\n",
    "            if val_loss < self.best_loss - self.mindelta:\n",
    "                self.best_loss = val_loss\n",
    "                self.counter = 0\n",
    "                self.best_model_state = model.state_dict()\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        elif self.mode == \"max\":\n",
    "            if val_loss > self.best_loss + self.mindelta:\n",
    "                self.best_loss = val_loss\n",
    "                self.counter = 0\n",
    "                self.best_model_state = model.state_dict()\n",
    "            else:\n",
    "                self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "\n",
    "    def restore_best_weights(self, model):\n",
    "        if self.best_model_state is not None and self.restore_best_weights_flag:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "\n",
    "def run_network(X_train, X_val, y_train, y_val, hyperparameters, seed):\n",
    "    input_dim = X_train.shape[2]\n",
    "    model = Mamba2Simple(\n",
    "        input_dim=input_dim,\n",
    "        d_model=hyperparameters['d_model'],\n",
    "        d_state=hyperparameters['d_state'],\n",
    "        d_inner=hyperparameters['d_inner'],\n",
    "        n_heads=hyperparameters['n_heads'],\n",
    "        bias=True\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hyperparameters['lr_scheduler'])\n",
    "\n",
    "    # Convert data to tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=hyperparameters['batch_size'], shuffle=False)\n",
    "\n",
    "    n_epochs_max = hyperparameters['n_epochs_max']\n",
    "    patience = hyperparameters['patience']\n",
    "    verbose = hyperparameters['verbose']\n",
    "    mindelta = hyperparameters['mindelta']\n",
    "\n",
    "    earlystopping = EarlyStopping(patience=patience, mindelta=mindelta, restore_best_weights=True, mode=\"min\")\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(n_epochs_max):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_X)\n",
    "\n",
    "            # Ensure y_pred shape is (batch_size, 1)\n",
    "            y_pred = y_pred.view(-1, 1)\n",
    "            assert y_pred.shape == batch_y.shape, f\"Shape mismatch: {y_pred.shape} vs {batch_y.shape}\"\n",
    "\n",
    "            loss = loss_fn(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                y_val_pred = model(batch_X)\n",
    "                y_val_pred = y_val_pred.view(-1, 1)\n",
    "                val_loss = loss_fn(y_val_pred, batch_y)\n",
    "                epoch_val_loss += val_loss.item()\n",
    "\n",
    "        epoch_val_loss /= len(val_loader)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "        earlystopping(epoch_val_loss, model)\n",
    "\n",
    "        if earlystopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            earlystopping.restore_best_weights(model)\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs_max} - Train Loss: {epoch_train_loss:.4f} - Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    history = {\n",
    "        'loss': train_loss_history,\n",
    "        'val_loss': val_loss_history\n",
    "    }\n",
    "\n",
    "    return model, history, earlystopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea8deb-5ba4-49b6-896e-256e01eab778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization with Optuna.    \n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Trial {trial.number} started\")\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "\n",
    "    hyperparameters_copy[\"dropout\"] = trial.suggest_float('dropout', 0.0, 0.3)\n",
    "    hyperparameters_copy[\"lr_scheduler\"] = trial.suggest_loguniform('lr_scheduler', 1e-3, 1e-1)\n",
    "    hyperparameters_copy['activation'] = trial.suggest_categorical(\"activation\", ['LeakyReLU', 'tanh'])\n",
    "    hyperparameters_copy['patience'] = trial.suggest_int('patience', 1, 50)\n",
    "    hyperparameters_copy['mindelta'] = trial.suggest_loguniform('mindelta', 1e-10, 1e-3)\n",
    "    hyperparameters_copy['weight_decay'] = trial.suggest_loguniform('weight_decay', 1e-5, 0)\n",
    "    common_value = trial.suggest_categorical('d_common', [32, 64])\n",
    "    hyperparameters_copy['d_model'] = common_value\n",
    "    hyperparameters_copy['d_state'] = common_value\n",
    "    hyperparameters_copy['d_inner'] = common_value\n",
    "    hyperparameters_copy['batch_size'] = hyperparameters['batch_size']\n",
    "    hyperparameters_copy['n_epochs_max'] = hyperparameters['n_epochs_max']\n",
    "    \n",
    "    v_val_loss = []\n",
    "\n",
    "    model, hist, earlystopping = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters_copy,\n",
    "            seed\n",
    "    )\n",
    "\n",
    "    v_val_loss.append(np.min(hist[\"val_loss\"]))\n",
    "\n",
    "    metric_dev = np.mean(v_val_loss)\n",
    "    return metric_dev\n",
    "\n",
    "def optuna_study(hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Find the best hyperparameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize') \n",
    "    study.optimize(lambda trial: objective(trial, hyperparameters, seed, X_train, y_train , X_val, y_val, split, norm, n_time_steps), n_trials=30)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_metric = study.best_value\n",
    "    \n",
    "    best_hyperparameters = {\n",
    "                'dropout': best_params['dropout'],\n",
    "                'lr_scheduler': best_params['lr_scheduler'],\n",
    "                'activation': best_params['activation'],\n",
    "                'batch_size': hyperparameters['batch_size'],\n",
    "                'n_epochs_max': hyperparameters['n_epochs_max'],\n",
    "                'patience': best_params['patience'],\n",
    "                'mindelta': best_params['mindelta'],\n",
    "                'd_common': best_params['d_common'],\n",
    "        \n",
    "                'weight_decay': best_params['weight_decay']\n",
    "            }\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Validation Metric: {best_metric}\")\n",
    "    \n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0bc5c-23a0-4eb1-bb80-334892884d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42, 76, 124]\n",
    "input_dim = 80  \n",
    "n_time_steps = 14\n",
    "batch_size = 32\n",
    "n_epochs_max = 1000\n",
    "\n",
    "norm = \"robustNorm\" \n",
    "\n",
    "# Mamba \n",
    "d_model = 64 \n",
    "d_state = 64  \n",
    "d_inner = 64  \n",
    "n_heads = 14  \n",
    "patience = 15  # Epochs for Early Stopping\n",
    "monitor = \"val_loss\"  \n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_time_steps\": n_time_steps,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs_max\": n_epochs_max,\n",
    "    \"patience\": patience,\n",
    "    \"monitor\": monitor,\n",
    "    \"mindelta\": 0,  # Minimum delta for Early Stopping\n",
    "    \"dropout\": 0.0, \n",
    "    \"verbose\": 0, \n",
    "    \"d_model\": d_model,\n",
    "    \"d_state\": d_state,\n",
    "    \"d_inner\": d_inner,\n",
    "    \"n_heads\": n_heads\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d026f8-bd2b-4158-a3bd-9672538bf460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model = True\n",
    "debug = True\n",
    "tab='\\t'\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "if run_model:\n",
    "    import time\n",
    "\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    training_times = []\n",
    "    optimization_times = []\n",
    "    inference_times = []\n",
    "\n",
    "    v_accuracy_test = []\n",
    "    v_specificity = []\n",
    "    v_precision = []\n",
    "    v_recall = []\n",
    "    v_f1score = []\n",
    "    v_roc = []\n",
    "    v_aucpr = []\n",
    "\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "    results = \"\"\n",
    "\n",
    "    for i in [1, 2, 3]:\n",
    "        print(\"====================>\", i)\n",
    "\n",
    "\n",
    "        X_train = np.load(f\"../../DATA/s{i}/X_train_tensor_robustNorm.npy\")\n",
    "        X_val = np.load(f\"../../DATA/s{i}/X_val_tensor_robustNorm.npy\")\n",
    "\n",
    "        y_train = pd.read_csv(f\"../../DATA/s{i}/y_train_robustNorm.csv\")[['individualMRGerm_stac']]\n",
    "        y_train = y_train.iloc[0:y_train.shape[0]:hyperparameters[\"n_time_steps\"]].reset_index(drop=True)\n",
    "\n",
    "        y_val = pd.read_csv(f\"../../DATA/s{i}/y_val_robustNorm.csv\")[['individualMRGerm_stac']]\n",
    "        y_val = y_val.iloc[0:y_val.shape[0]:hyperparameters[\"n_time_steps\"]].reset_index(drop=True)\n",
    "\n",
    "        X_test = np.load(f\"../../DATA/s{i}/X_test_tensor_robustNorm.npy\")\n",
    "        \n",
    "        y_test = pd.read_csv(f\"../../DATA/s{i}/y_test_robustNorm.csv\")[['individualMRGerm_stac']]\n",
    "        y_test = y_test.iloc[0:y_test.shape[0]:hyperparameters[\"n_time_steps\"]].reset_index(drop=True)\n",
    "        y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "        start_opt = time.time()\n",
    "        bestHyperparameters = optuna_study(\n",
    "            hyperparameters,\n",
    "            seeds[i-1],\n",
    "            X_train, y_train,  \n",
    "            X_val, y_val,\n",
    "            f\"s{i}\",\n",
    "            norm,\n",
    "            n_time_steps\n",
    "        )\n",
    "        end_opt = time.time()\n",
    "        optimization_times.append(end_opt - start_opt)\n",
    "\n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "\n",
    "\n",
    "        split_directory = f'./Results_Mamba_optuna/split_{i}'\n",
    "        os.makedirs(split_directory, exist_ok=True)\n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters.update({\n",
    "            \"dropout\": bestHyperparameters[\"dropout\"],\n",
    "            \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"],\n",
    "            \"batch_size\": bestHyperparameters[\"batch_size\"],\n",
    "            \"n_epochs_max\": bestHyperparameters[\"n_epochs_max\"],\n",
    "            \"patience\": bestHyperparameters[\"patience\"],\n",
    "            'd_model': bestHyperparameters['d_common'],\n",
    "            'd_state': bestHyperparameters['d_common'],\n",
    "            'd_inner': bestHyperparameters['d_common'],\n",
    "        })\n",
    "\n",
    "\n",
    "        reset_pytorch()\n",
    "        print(hyperparameters)\n",
    "\n",
    "\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "        t0_train = time.time()\n",
    "\n",
    "        model, history, earlystopping = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train, y_val,\n",
    "            hyperparameters,\n",
    "            seeds[i - 1]\n",
    "        )\n",
    "\n",
    "        t1_train = time.time()\n",
    "        training_times.append(t1_train - t0_train)\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(history['loss'])\n",
    "        loss_dev.append(history['val_loss'])\n",
    "\n",
    "        # ----- INFERENCE -----\n",
    "        t0_inf = time.time()\n",
    "\n",
    "        y_pred = model(X_test_tensor).detach().cpu().numpy()\n",
    "\n",
    "        t1_inf = time.time()\n",
    "        inference_times.append(t1_inf - t0_inf)\n",
    "\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "\n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "\n",
    "        model_filename = os.path.join(split_directory, f\"model_split_{i}.h5\")\n",
    "        torch.save(model, model_filename)\n",
    "\n",
    "        # ----- METRICS -----\n",
    "        accuracy_test = accuracy_score(y_test, np.round(y_pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, np.round(y_pred)).ravel()\n",
    "        roc = roc_auc_score(y_test, y_pred)\n",
    "        aucpr = average_precision_score(y_test, y_pred)\n",
    "\n",
    "        v_accuracy_test.append(accuracy_test)\n",
    "        v_specificity.append(tn / (tn + fp))\n",
    "        v_precision.append(tp / (tp + fp))\n",
    "        v_recall.append(tp / (tp + fn))\n",
    "        v_f1score.append((2 * v_recall[i - 1] * v_precision[i - 1]) / (v_recall[i - 1] + v_precision[i - 1]))\n",
    "        v_roc.append(roc)\n",
    "        v_aucpr.append(aucpr)\n",
    "\n",
    "        if debug:\n",
    "            results += tab + f\"Split {i} - Timing (s):\\n\"\n",
    "            results += tab + f\"{tab}Optimization: {optimization_times[-1]:.2f}\\n\"\n",
    "            results += tab + f\"{tab}Training: {training_times[-1]:.2f}\\n\"\n",
    "            results += tab + f\"{tab}Inference: {inference_times[-1]:.2f}\\n\"\n",
    "            results += tab + f\"\\tTP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}\\n\"\n",
    "            results += tab + f\"\\tAccuracy: {accuracy_test:.4f} | ROC-AUC: {roc:.4f} | AUC-PR: {aucpr:.4f}\\n\"\n",
    "\n",
    "    # SAVE\n",
    "    directory = './Results_Mamba_optuna'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Split\": [i for i in range(1, len(v_accuracy_test) + 1)],\n",
    "        \"OptimizationTime\": optimization_times,\n",
    "        \"TrainingTime\": training_times,\n",
    "        \"InferenceTime\": inference_times,\n",
    "        \"Accuracy\": v_accuracy_test,\n",
    "        \"Specificity\": v_specificity,\n",
    "        \"Precision\": v_precision,\n",
    "        \"Recall\": v_recall,\n",
    "        \"F1Score\": v_f1score,\n",
    "        \"ROC_AUC\": v_roc,\n",
    "        \"AUC_PR\": v_aucpr\n",
    "    })\n",
    "\n",
    "    summary_path = os.path.join(directory, \"summary_metrics.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n--- SUMMARY ---\")\n",
    "        print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8257e1-7aac-4368-bb9e-4f898bec6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 64.15 +- 4.08\n",
      "Specificity: 87.77 +- 0.16\n",
      "Precision: 48.30 +- 1.69\n",
      "F1-score: 55.09 +- 2.59\n",
      "ROC-AUC: 79.40 +- 3.58\n",
      "AUC-PR: 49.09 +- 3.61\n",
      "Test Accuracy: 84.19 +- 0.67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = './Results_Mamba_optuna'\n",
    "summary_path = os.path.join(directory, \"summary_metrics.csv\")\n",
    "summary_df = pd.read_csv(summary_path)\n",
    "\n",
    "\n",
    "def calculateKPI(parameter):\n",
    "    \"\"\"\n",
    "    This function calculate the mean and deviation of a set of values of\n",
    "    a given performance indicator.\n",
    "    \n",
    "    Returns: Mean and std (float)\n",
    "    \"\"\"\n",
    "    mean = round(np.mean(parameter)*100, 2)\n",
    "    deviation = round(np.sqrt(np.sum(np.power(parameter - np.mean(parameter), 2) / len(parameter)))*100, 2)\n",
    "    return mean, deviation\n",
    "\n",
    "def format_metric_line(metric_name, mean_value, deviation_value):\n",
    "    return f\"{metric_name}: {mean_value:.2f} +- {deviation_value:.2f}\\n\"\n",
    "\n",
    "mean_test, deviation_test = calculateKPI(summary_df[\"Accuracy\"])\n",
    "mean_specificity, deviation_specificity = calculateKPI(summary_df[\"Specificity\"])\n",
    "mean_recall, deviation_recall = calculateKPI(summary_df[\"Recall\"])\n",
    "mean_f1, deviation_f1 = calculateKPI(summary_df[\"F1Score\"])\n",
    "mean_precision, deviation_precision = calculateKPI(summary_df[\"Precision\"])\n",
    "mean_roc, deviation_roc = calculateKPI(summary_df[\"ROC_AUC\"])\n",
    "mean_aucpr, deviation_aucpr = calculateKPI(summary_df[\"AUC_PR\"])  \n",
    "\n",
    "results = \"\"\n",
    "results += format_metric_line(\"Test Accuracy\", mean_test, deviation_test)\n",
    "results += format_metric_line(\"Specificity\", mean_specificity, deviation_specificity)\n",
    "results += format_metric_line(\"Sensitivity\", mean_recall, deviation_recall)\n",
    "results += format_metric_line(\"Precision\", mean_precision, deviation_precision)\n",
    "results += format_metric_line(\"F1-score\", mean_f1, deviation_f1)\n",
    "results += format_metric_line(\"ROC-AUC\", mean_roc, deviation_roc)\n",
    "results += format_metric_line(\"AUC-PR\", mean_aucpr, deviation_aucpr) \n",
    "\n",
    "final_results = (\n",
    "    f\"Sensitivity: {mean_recall:.2f} +- {deviation_recall:.2f}\\n\"\n",
    "    f\"Specificity: {mean_specificity:.2f} +- {deviation_specificity:.2f}\\n\"\n",
    "    f\"Precision: {mean_precision:.2f} +- {deviation_precision:.2f}\\n\"\n",
    "    f\"F1-score: {mean_f1:.2f} +- {deviation_f1:.2f}\\n\"\n",
    "    f\"ROC-AUC: {mean_roc:.2f} +- {deviation_roc:.2f}\\n\"\n",
    "    f\"AUC-PR: {mean_aucpr:.2f} +- {deviation_aucpr:.2f}\\n\" \n",
    "    f\"Test Accuracy: {mean_test:.2f} +- {deviation_test:.2f}\\n\"\n",
    ")\n",
    "\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ecf16d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
