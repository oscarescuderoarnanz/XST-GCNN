{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import random, os, json\n",
    "# Configurar variables de entorno\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # \"-1\" significa deshabilitar todas las GPUs\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, GRU, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateKPI(parameter):\n",
    "    \"\"\"\n",
    "    This function calculate the mean and deviation of a set of values of\n",
    "    a given performance indicator.\n",
    "    \n",
    "    Returns: Mean and std (float)\n",
    "    \"\"\"\n",
    "    mean = round(np.mean(parameter)*100, 2)\n",
    "    deviation = round(np.sqrt(np.sum(np.power(parameter - np.mean(parameter), 2) / len(parameter)))*100, 2)\n",
    "    return mean, deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_keras(seed=42):\n",
    "    \"\"\"Function to ensure that results from Keras models\n",
    "    are consistent and reproducible across different runs\"\"\"\n",
    "    \n",
    "    K.clear_session()\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed)\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed)\n",
    "    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparameters):\n",
    "    \"\"\"\n",
    "    Builds a LSTM model based on several hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - hyperparameters: Dictionary containing the hyperparameters. \n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the compiled model.\n",
    "    \"\"\"\n",
    "    \n",
    "    dynamic_input = tf.keras.layers.Input(shape=(hyperparameters[\"n_time_steps\"], hyperparameters[\"layers\"][0]))\n",
    "    masked = tf.keras.layers.Masking(mask_value=hyperparameters['mask_value'])(dynamic_input)\n",
    "\n",
    "    gru = tf.keras.layers.GRU(\n",
    "        hyperparameters[\"layers\"][1],\n",
    "        dropout=hyperparameters['dropout'],\n",
    "        return_sequences=False,\n",
    "        activation=hyperparameters['activation'],\n",
    "        use_bias=True\n",
    "    )(masked)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, use_bias=True, activation=\"sigmoid\")(gru)\n",
    "\n",
    "    model = tf.keras.Model(dynamic_input, [output])\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters[\"lr_scheduler\"]),\n",
    "        metrics=['accuracy', \"AUC\"]\n",
    "    )\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(X_train, X_val, y_train, y_val, hyperparameters, seed):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the built LSTM model based on the provided data and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - sample_weights_train, sample_weights_val: numpy.ndarray. Weights for the T and V data to handle class imbalance.\n",
    "        - hyperparameters: Dictionary containing the hyperparameters.\n",
    "        - seed: Integer seed for reproducibility.\n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the trained model.\n",
    "        - hist:  The training history.\n",
    "        - earlystopping: The early stopping callback.\n",
    "    \"\"\"\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    n_epochs_max = hyperparameters['n_epochs_max']    \n",
    "\n",
    "    model = None\n",
    "    model = build_model(hyperparameters)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  min_delta=hyperparameters[\"mindelta\"],\n",
    "                                                  patience=hyperparameters[\"patience\"],\n",
    "                                                  restore_best_weights=True,\n",
    "                                                  mode=\"min\")\n",
    "    hist = model.fit(X_train, y_train,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     callbacks=[earlystopping], batch_size=batch_size, epochs=n_epochs_max,\n",
    "                     verbose=hyperparameters['verbose'])\n",
    "    \n",
    "    return model, hist, earlystopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_combination(k, l, m, b, hyperparameters, dropout, layers, lr_scheduler, activation, seed, split, norm, n_time_steps):\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "    hyperparameters_copy['dropout'] = dropout[k]\n",
    "    hyperparameters_copy['layers'] = layers[l]\n",
    "    hyperparameters_copy['lr_scheduler'] = lr_scheduler[m]\n",
    "    hyperparameters_copy['activation'] = activation[b]\n",
    "    \n",
    "    v_val_loss = []\n",
    "    \n",
    "    X_train = np.load(\"../../DATA/s\" + str(i) + \"/X_train_tensor_normPower2\" + \".npy\")\n",
    "    X_val = np.load(\"../../DATA/s\" + str(i) + \"/X_val_tensor_normPower2\" + \".npy\")\n",
    "\n",
    "    y_train = pd.read_csv(\"../../DATA/s\" + str(i) + \"/y_train_normPower2\" + \".csv\")\n",
    "    y_train = y_train[['individualMRGerm_stac']]\n",
    "    y_train = y_train.iloc[0:y_train.shape[0]:hyperparameters[\"n_time_steps\"]].reset_index(drop=True)\n",
    "\n",
    "    y_val = pd.read_csv(\"../../DATA/s\" + str(i) + \"/y_val_normPower2\" + \".csv\")\n",
    "    y_val = y_val[['individualMRGerm_stac']]\n",
    "    y_val = y_val.iloc[0:y_val.shape[0]:hyperparameters[\"n_time_steps\"]].reset_index(drop=True)\n",
    "    \n",
    "    reset_keras()\n",
    "\n",
    "    model, hist, early = run_network(\n",
    "        X_train, X_val,\n",
    "        y_train,\n",
    "        y_val,\n",
    "        hyperparameters_copy,\n",
    "        seed\n",
    "    )\n",
    "\n",
    "    v_val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "\n",
    "    metric_dev = np.mean(v_val_loss)\n",
    "    return (metric_dev, k, l, m, b, X_train, y_train, X_val, y_val)\n",
    "\n",
    "def myCVGridParallel(hyperparameters, dropout, lr_scheduler, layers, activation, seed, split, norm, n_time_steps=14):\n",
    "    \"\"\"Parallelized Grid Search. \n",
    "       Calculate metricDev based on the evaluation. Compares the metricDev with the current bestMetricDev. \n",
    "       If better, updates bestMetricDev and stores those hyperparameters in bestHyperparameters.\n",
    "       \n",
    "    Args:\n",
    "        - hyperparameters: Dictionary containing the hyperparameters.\n",
    "        - dropout: A list of dropout rates.\n",
    "        - lr_scheduler: A list of learning rates.\n",
    "        - layers: A list of layer configurations.\n",
    "        - seed : Seed value for reproducibility.\n",
    "        - split: String indicating the data split.\n",
    "        - norm: String with the type of normalization applied to the data.\n",
    "    Returns:\n",
    "        - bestHyperparameters: A dictionary with the best hyperparameters found and Train and Val data.\n",
    "    \"\"\"\n",
    "    bestHyperparameters = {}\n",
    "    bestMetricDev = np.inf\n",
    "\n",
    "\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(evaluate_combination)(k, l, m, b, hyperparameters, dropout, layers, lr_scheduler, activation, seed, split, norm, n_time_steps)\n",
    "        for k in range(len(dropout))\n",
    "        for l in range(len(layers))\n",
    "        for m in range(len(lr_scheduler))\n",
    "        for b in range(len(activation))\n",
    "    )\n",
    "\n",
    "    for metric_dev, k, l, m, b, X_train, y_train, X_val, y_val in results:\n",
    "        if metric_dev < bestMetricDev:\n",
    "            bestMetricDev = metric_dev\n",
    "            bestHyperparameters = {\n",
    "                'dropout': dropout[k],\n",
    "                'layers': layers[l],\n",
    "                'lr_scheduler': lr_scheduler[m],\n",
    "                'activation': activation[b],\n",
    "                'X_train': X_train,\n",
    "                'y_train': y_train,\n",
    "                'X_val': X_val,\n",
    "                'y_val': y_val\n",
    "            }\n",
    "\n",
    "    return bestHyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "In the dictionary, hyperparameters related to: data, training, evaluation, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42, 76, 124, 163, 192, 205]\n",
    "\n",
    "input_shape = 80\n",
    "n_time_steps = 14\n",
    "batch_size = 32\n",
    "n_epochs_max = 1000\n",
    "\n",
    "layer_list = [\n",
    "    [input_shape, 20, 1],  [input_shape, 30, 1], [input_shape, 35, 1], \n",
    "    [input_shape, 40, 1], [input_shape, 45, 1], [input_shape, 50, 1]\n",
    "]\n",
    "\n",
    "dropout = [0, .15, .3]\n",
    "lr_scheduler = [1e-1, 1e-2, 1e-3]\n",
    "\n",
    "activation = ['LeakyReLU']\n",
    " \n",
    "norm = \"robustNorm\"\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_time_steps\": n_time_steps,\n",
    "    \"mask_value\": 666,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs_max\": n_epochs_max,\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": 50,\n",
    "    \"dropout\": 0.0,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "tab = \"\\t\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 3ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "run_model = True\n",
    "debug = True\n",
    "\n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    v_accuracy_test = []\n",
    "    v_specificity = []\n",
    "    v_precision = []\n",
    "    v_recall = []\n",
    "    v_f1score = []\n",
    "    v_roc = []\n",
    "    v_early = []\n",
    "    v_probs = []\n",
    "    results = \"\"\n",
    "\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "\n",
    "    for i in [1,2,3]:\n",
    "        init = time.time()\n",
    "        # LOAD TEST AND PRE-TRAIN\n",
    "        X_test = np.load(\"../../DATA/s\" + str(i) + \"/X_test_tensor_normPower2\" + \".npy\")\n",
    "\n",
    "        y_test = pd.read_csv(\"../../DATA/s\" + str(i) + \"/y_test_normPower2\" + \".csv\")\n",
    "        y_test = y_test[['individualMRGerm_stac']]\n",
    "        y_test = y_test.iloc[0:y_test.shape[0]:hyperparameters[\"n_time_steps\"]].reset_index(drop=True)\n",
    "\n",
    "        # GridSearch of hyperparameters \n",
    "        bestHyperparameters = myCVGridParallel(hyperparameters,\n",
    "                                               dropout,\n",
    "                                               lr_scheduler,\n",
    "                                               layer_list,\n",
    "                                               activation,\n",
    "                                               seeds[i],\n",
    "                                               \"s\"+str(i),\n",
    "                                               norm)\n",
    "        fin = time.time()\n",
    "        X_train = bestHyperparameters[\"X_train\"]\n",
    "        y_train = bestHyperparameters[\"y_train\"]\n",
    "        X_val = bestHyperparameters[\"X_val\"]\n",
    "        y_val = bestHyperparameters[\"y_val\"]\n",
    "\n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "\n",
    "        # Save best hyperparameters for current split\n",
    "        split_directory = './Results_GRU/split_' + str(i)\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "\n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters = {\n",
    "            'n_time_steps': hyperparameters[\"n_time_steps\"],\n",
    "            'mask_value': hyperparameters[\"mask_value\"],\n",
    "\n",
    "            'batch_size': hyperparameters[\"batch_size\"],\n",
    "            'n_epochs_max': hyperparameters[\"n_epochs_max\"],\n",
    "            'monitor':  hyperparameters[\"monitor\"],\n",
    "            \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "            \"patience\": hyperparameters[\"patience\"],\n",
    "            \"dropout\": bestHyperparameters[\"dropout\"],\n",
    "            \"layers\": bestHyperparameters[\"layers\"],\n",
    "            \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"],\n",
    "            \"activation\": bestHyperparameters[\"activation\"],\n",
    "            'verbose': 0\n",
    "        }\n",
    "\n",
    "        # --- TRY ON TEST ----------------------------------------------------------------------\n",
    "        reset_keras()\n",
    "\n",
    "        model, hist, early = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters,\n",
    "            seeds[i-1]\n",
    "        )\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(hist.history['loss'])\n",
    "        loss_dev.append(hist.history['val_loss'])\n",
    "\n",
    "        y_pred = model.predict(x=X_test)\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "\n",
    "        # Save y_pred for current split\n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "\n",
    "        # Save model for current split\n",
    "        model_filename = os.path.join(split_directory, f\"model_split_{i}.h5\")\n",
    "        model.save(model_filename)\n",
    "        \n",
    "    \n",
    "        accuracy_test = sklearn.metrics.accuracy_score(y_test, np.round(y_pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, np.round(y_pred)).ravel()\n",
    "        roc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        v_accuracy_test.append(accuracy_test)\n",
    "        v_specificity.append(tn / (tn + fp))\n",
    "        v_precision.append(tp / (tp + fp))\n",
    "        v_recall.append(tp / (tp + fn))\n",
    "        v_f1score.append((2 * v_recall[i-1] * v_precision[i-1]) / (v_recall[i-1] + v_precision[i-1]))\n",
    "        v_roc.append(roc)\n",
    "\n",
    "        if debug:\n",
    "            results = results + tab + \"\\tPositivos bien predichos\" + str(tp) + \"\\n\"\n",
    "            results = results + tab + \"\\tPositivos mal predichos\" + str(fp) + \"\\n\"\n",
    "            results = results + tab + \"\\tNegativos bien predichos\" + str(tn) + \"\\n\"\n",
    "            results = results + tab + \"\\tNegativos mal predichos\" + str(fn) + \"\\n\"\n",
    "        \n",
    "    \n",
    "\n",
    "    # END EXECUTION - SAVE AGGREGATED RESULTS\n",
    "    directory = './Results_GRU'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    def save_to_pickle(data, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    save_to_pickle(bestHyperparameters_bySplit, os.path.join(directory, \"bestHyperparameters_bySplit.pkl\"))\n",
    "    save_to_pickle(y_pred_by_split, os.path.join(directory, \"y_pred_by_split.pkl\"))\n",
    "    \n",
    "    for i, model in enumerate(v_models):\n",
    "        model_filename = os.path.join(directory, f\"model_{i}.h5\")\n",
    "        model.save(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 58.49 +- 4.08\n",
      "Specificity: 93.04 +- 2.48\n",
      "Precision: 61.29 +- 7.25\n",
      "F1-score: 59.36 +- 1.66\n",
      "ROC-AUC: 80.78 +- 1.57\n",
      "Test Accuracy: 87.81 +- 1.52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_metric_line(metric_name, mean_value, deviation_value):\n",
    "    return f\"{metric_name}: {mean_value:.2f} +- {deviation_value:.2f}\\n\"\n",
    "\n",
    "# Calculate the metrics\n",
    "mean_test, deviation_test = calculateKPI(v_accuracy_test)\n",
    "mean_specificity, deviation_specificity = calculateKPI(v_specificity)\n",
    "mean_recall, deviation_recall = calculateKPI(v_recall)\n",
    "mean_f1, deviation_f1 = calculateKPI(v_f1score)\n",
    "mean_precision, deviation_precision = calculateKPI(v_precision)\n",
    "mean_roc, deviation_roc = calculateKPI(v_roc)\n",
    "\n",
    "# Generate the results string\n",
    "results = \"\"\n",
    "results += format_metric_line(\"Test Accuracy\", mean_test, deviation_test)\n",
    "results += format_metric_line(\"Specificity\", mean_specificity, deviation_specificity)\n",
    "results += format_metric_line(\"Sensitivity\", mean_recall, deviation_recall)\n",
    "results += format_metric_line(\"Precision\", mean_precision, deviation_precision)\n",
    "results += format_metric_line(\"F1-score\", mean_f1, deviation_f1)\n",
    "results += format_metric_line(\"ROC-AUC\", mean_roc, deviation_roc)\n",
    "\n",
    "# Final formatted string for all metrics\n",
    "final_results = (\n",
    "    f\"Sensitivity: {mean_recall:.2f} +- {deviation_recall:.2f}\\n\"\n",
    "    f\"Specificity: {mean_specificity:.2f} +- {deviation_specificity:.2f}\\n\"\n",
    "    f\"Precision: {mean_precision:.2f} +- {deviation_precision:.2f}\\n\"\n",
    "    f\"F1-score: {mean_f1:.2f} +- {deviation_f1:.2f}\\n\"\n",
    "    f\"ROC-AUC: {mean_roc:.2f} +- {deviation_roc:.2f}\\n\"\n",
    "    f\"Test Accuracy: {mean_test:.2f} +- {deviation_test:.2f}\\n\"\n",
    ")\n",
    "\n",
    "# Optionally, you can print or log the results\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
