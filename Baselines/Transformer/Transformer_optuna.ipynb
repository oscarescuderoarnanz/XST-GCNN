{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "059dfa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, Dropout, Dense, Dropout, Flatten, Conv1D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b658ca",
   "metadata": {},
   "source": [
    "# FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb714b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Masking, LayerNormalization, MultiHeadAttention,\n",
    "    Dropout, Dense, GlobalMaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def reset_keras(seed=42):\n",
    "    \"\"\"Ensure reproducibility across runs, safely handling thread config.\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Try to enforce single-threaded behavior if not already initialized\n",
    "    try:\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    except RuntimeError:\n",
    "        # If TF threading config is already set, ignore the error\n",
    "        pass\n",
    "\n",
    "def get_sinusoidal_encoding(seq_len, dim):\n",
    "    \"\"\"Generate sinusoidal positional encoding.\"\"\"\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, dim, 2) * -(np.log(10000.0) / dim))\n",
    "    pe = np.zeros((seq_len, dim))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return tf.constant(pe[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "def build_model(hyperparameters):\n",
    "    \"\"\"Build Transformer with positional encoding and proper block chaining.\"\"\"\n",
    "    # Unpack hyperparameters\n",
    "    seq_len = hyperparameters[\"n_time_steps\"]\n",
    "    embed_dim = hyperparameters[\"layers\"][0]\n",
    "    dropout = hyperparameters[\"dropout\"]\n",
    "    num_heads = hyperparameters[\"num_heads\"]\n",
    "    num_blocks = hyperparameters[\"num_transformer_blocks\"]\n",
    "    activation = hyperparameters[\"activation\"]\n",
    "    l2_reg = hyperparameters.get('l2_reg', 1e-4)\n",
    "    epsilon = hyperparameters.get('epsilon', 1e-6)\n",
    "\n",
    "    # Inputs\n",
    "    inputs = Input(shape=(seq_len, embed_dim))\n",
    "    x = Masking(mask_value=hyperparameters[\"mask_value\"])(inputs)\n",
    "\n",
    "    # Positional Encoding\n",
    "    pos_encoding = get_sinusoidal_encoding(seq_len, embed_dim)\n",
    "    x = x + pos_encoding\n",
    "\n",
    "    # Transformer Blocks\n",
    "    for _ in range(num_blocks):\n",
    "        # Self-Attention\n",
    "        x_norm = LayerNormalization(epsilon=epsilon)(x)\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x_norm, x_norm)\n",
    "        attn_output = Dropout(dropout)(attn_output)\n",
    "        x = attn_output + x\n",
    "\n",
    "        # Feed-forward\n",
    "        x_norm = LayerNormalization(epsilon=epsilon)(x)\n",
    "        ffn_output = Dense(\n",
    "            embed_dim, activation=activation,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        )(x_norm)\n",
    "        x = ffn_output + x\n",
    "\n",
    "    # Output layers\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(\n",
    "        hyperparameters[\"layers\"][1], activation=activation,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "    )(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    output = GlobalMaxPooling1D()(x)\n",
    "    output = Dense(1, activation='sigmoid',\n",
    "                   kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(output)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=hyperparameters[\"lr_scheduler\"]),\n",
    "        metrics=['accuracy', 'AUC']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def run_network(X_train, X_val, y_train, y_val, hyperparameters):\n",
    "    \"\"\"Train and evaluate model using float32.\"\"\"\n",
    "    model = build_model(hyperparameters)\n",
    "\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=hyperparameters[\"mindelta\"],\n",
    "        patience=hyperparameters[\"patience\"],\n",
    "        restore_best_weights=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    # Ensure float32 dtype\n",
    "    X_train = tf.cast(X_train, tf.float32)\n",
    "    y_train = tf.cast(y_train, tf.float32)\n",
    "    X_val = tf.cast(X_val, tf.float32)\n",
    "    y_val = tf.cast(y_val, tf.float32)\n",
    "\n",
    "    start = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[earlystopping],\n",
    "        batch_size=hyperparameters['batch_size'],\n",
    "        epochs=hyperparameters['n_epochs_max'],\n",
    "        verbose=hyperparameters[\"verbose\"],\n",
    "    )\n",
    "    train_time = time.time() - start\n",
    "    return model, history, train_time\n",
    "\n",
    "def objective(trial, base_hparams, seed, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Optuna objective with reproducibility.\"\"\"\n",
    "    reset_keras(seed)\n",
    "    # Copy base and sample\n",
    "    hparams = base_hparams.copy()\n",
    "    hparams[\"dropout\"] = trial.suggest_float('dropout', 0.0, 0.3)\n",
    "    hparams['middle_layer_dim'] = trial.suggest_int('middle_layer_dim', 1, 40, step=2)\n",
    "    hparams['layers'] = [hparams['layers'][0], hparams['middle_layer_dim'], 1]\n",
    "    hparams[\"lr_scheduler\"] = trial.suggest_loguniform('lr_scheduler', 1e-3, 1e-1)\n",
    "    hparams['l2_reg'] = trial.suggest_loguniform('l2_reg', 1e-6, 1e-2)\n",
    "    hparams['num_transformer_blocks'] = trial.suggest_int(\"num_transformer_blocks\", 1, 3)\n",
    "    hparams['activation'] = trial.suggest_categorical(\"activation\", ['LeakyReLU'])\n",
    "    hparams['num_heads'] = trial.suggest_int(\"num_heads\", 2, 3)\n",
    "    hparams['epsilon'] = trial.suggest_loguniform('epsilon', 1e-6, 1e-6)\n",
    "    hparams['patience'] = trial.suggest_int('patience', 1, 50)\n",
    "    hparams['mindelta'] = trial.suggest_loguniform('mindelta', 1e-10, 1e-3)\n",
    "    hparams['weight_decay'] = trial.suggest_loguniform('weight_decay', 1e-5, 0)\n",
    "\n",
    "    model, history, _ = run_network(X_train, X_val, y_train, y_val, hparams)\n",
    "    return min(history.history[\"val_loss\"])\n",
    "\n",
    "def optuna_study(base_hparams, seed, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Run Optuna with TPESampler for reproducibility.\"\"\"\n",
    "    sampler = TPESampler(seed=seed)\n",
    "    study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "    study.optimize(lambda t: objective(\n",
    "        t, base_hparams, seed, X_train, y_train, X_val, y_val\n",
    "    ), n_trials=30)\n",
    "    return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447021ef",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe3c500",
   "metadata": {},
   "source": [
    "- **seeds**: Seed values to ensure reproducibility.\n",
    "- **input_shape**: Number of features in each time step of the input data.\n",
    "- **n_time_steps**: Number of time steps in the input sequence.\n",
    "- **batch_size**: Number of batches for training.\n",
    "- **n_epochs_max**: Maximum number of epochs for training.\n",
    "- **layer_list**: A list with different configurations for the layers of the model.\n",
    "- **dropout**: Dropout rates.\n",
    "- **lr_scheduler**: Learning rates.\n",
    "- **norm**: Type of normalization applied to the data.\n",
    "- **num_heads**: Number of attention heads in the multi-head attention mechanism.\n",
    "- **num_transformer_blocks**: Number of transformer blocks.\n",
    "- **epsilon**: Avoid zero division in the normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42, 76, 124, 163, 192, 205]\n",
    "\n",
    "adjustment_factor = [1] \n",
    "activation = ['LeakyReLU']\n",
    "norm = \"robustNorm\"\n",
    "patience = 3\n",
    "monitor = \"val_loss\" \n",
    "\n",
    "input_shape    = 80  \n",
    "n_time_steps   = 14\n",
    "batch_size     = 32\n",
    "n_epochs_max   = 1000\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_time_steps\": n_time_steps,\n",
    "    \"mask_value\": 666,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs_max\": n_epochs_max,\n",
    "    \"monitor\": monitor,\n",
    "    \"layers\": [input_shape, input_shape, 1],\n",
    "\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": patience,\n",
    "    \"dropout\": 0.2,\n",
    "    \"verbose\": 1,\n",
    "    \"input_shape\": input_shape,\n",
    "    \"num_heads\": 7,\n",
    "    \"num_transformer_blocks\": 0,\n",
    "    \"l2_reg\": 1e-4,\n",
    "    \"epsilon\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766690a0",
   "metadata": {},
   "source": [
    "# RUNNING AND TRYING ON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83a725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model = True\n",
    "debug = True\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    training_times = []\n",
    "    optimization_times = []\n",
    "    inference_times = []\n",
    "\n",
    "    v_accuracy_test = []\n",
    "    v_specificity = []\n",
    "    v_precision = []\n",
    "    v_recall = []\n",
    "    v_f1score = []\n",
    "    v_roc = []\n",
    "    v_aucpr = []\n",
    "\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "    results = \"\"\n",
    "\n",
    "    for i in [1, 2, 3]:\n",
    "        print(f\"\\n===== Split {i} =====\")\n",
    "\n",
    "        X_train = np.load(f\"../../DATA/s{i}/X_train_tensor_robustNorm.npy\")\n",
    "        X_val   = np.load(f\"../../DATA/s{i}/X_val_tensor_robustNorm.npy\")\n",
    "        X_test  = np.load(f\"../../DATA/s{i}/X_test_tensor_robustNorm.npy\")\n",
    "\n",
    "        y_train = pd.read_csv(f\"../../DATA/s{i}/y_train_robustNorm.csv\")[['individualMRGerm_stac']]\n",
    "        y_train = y_train.iloc[0::hyperparameters[\"n_time_steps\"]].reset_index(drop=True)\n",
    "        y_val   = pd.read_csv(f\"../../DATA/s{i}/y_val_robustNorm.csv\")[['individualMRGerm_stac']]\n",
    "        y_val   = y_val.iloc[0::hyperparameters[\"n_time_steps\"]].reset_index(drop=True)\n",
    "        y_test  = pd.read_csv(f\"../../DATA/s{i}/y_test_robustNorm.csv\")[['individualMRGerm_stac']]\n",
    "        y_test  = y_test.iloc[0::hyperparameters[\"n_time_steps\"]].reset_index(drop=True)\n",
    "\n",
    "        start_opt = time.time()\n",
    "        best_params, best_val = optuna_study(\n",
    "            base_hparams=hyperparameters,\n",
    "            seed=seeds[i-1],\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val,     y_val=y_val\n",
    "        )\n",
    "        end_opt = time.time()\n",
    "        optimization_times.append(end_opt - start_opt)\n",
    "\n",
    "        print(f\"Best params (val_loss={best_val:.4f}): {best_params}\")\n",
    "        bestHyperparameters_bySplit[str(i)] = best_params\n",
    "\n",
    "        embed_dim = hyperparameters[\"layers\"][0]\n",
    "        layers = [embed_dim, best_params[\"middle_layer_dim\"], 1]\n",
    "\n",
    "        train_hparams = hyperparameters.copy()\n",
    "        train_hparams.update({\n",
    "            \"dropout\":             best_params[\"dropout\"],\n",
    "            \"layers\":              layers,\n",
    "            \"lr_scheduler\":        best_params[\"lr_scheduler\"],\n",
    "            \"l2_reg\":              best_params[\"l2_reg\"],\n",
    "            \"num_transformer_blocks\": best_params[\"num_transformer_blocks\"],\n",
    "            \"activation\":          best_params[\"activation\"],\n",
    "            \"num_heads\":           best_params[\"num_heads\"],\n",
    "            \"epsilon\":             best_params[\"epsilon\"],\n",
    "            \"patience\":            best_params[\"patience\"],\n",
    "            \"mindelta\":            best_params[\"mindelta\"],\n",
    "        })\n",
    "\n",
    "        split_dir = f'./Results_Transformer_optuna/split_{i}'\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        with open(os.path.join(split_dir, f\"best_params_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(best_params, f)\n",
    "\n",
    "        reset_keras(seeds[i-1])\n",
    "        print(\"Train hyperparameters:\", train_hparams)\n",
    "\n",
    "        start_train = time.time()\n",
    "        model, hist, _time_train = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,   y_val,\n",
    "            train_hparams\n",
    "        )\n",
    "        end_train = time.time()\n",
    "        training_times.append(end_train - start_train)\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(hist.history['loss'])\n",
    "        loss_dev.append(hist.history['val_loss'])\n",
    "\n",
    "\n",
    "        start_inf = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        end_inf = time.time()\n",
    "        inference_times.append(end_inf - start_inf)\n",
    "\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "        with open(os.path.join(split_dir, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "\n",
    "        model.save(os.path.join(split_dir, f\"model_split_{i}.h5\"))\n",
    "\n",
    "        acc   = accuracy_score(y_test, np.round(y_pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, np.round(y_pred)).ravel()\n",
    "        roc   = roc_auc_score(y_test, y_pred)\n",
    "        aucpr = average_precision_score(y_test, y_pred)\n",
    "\n",
    "        v_accuracy_test.append(acc)\n",
    "        v_specificity.append(tn / (tn + fp))\n",
    "        v_precision.append(tp / (tp + fp))\n",
    "        v_recall.append(tp / (tp + fn))\n",
    "        v_f1score.append((2 * v_recall[-1] * v_precision[-1]) / (v_recall[-1] + v_precision[-1]))\n",
    "        v_roc.append(roc)\n",
    "        v_aucpr.append(aucpr)\n",
    "\n",
    "        if debug:\n",
    "            results += (\n",
    "                f\"Split {i} - Times (s): Opt {optimization_times[-1]:.2f}, \"\n",
    "                f\"Train {training_times[-1]:.2f}, Inf {inference_times[-1]:.2f}\\n\"\n",
    "                f\"    TP {tp} | FP {fp} | TN {tn} | FN {fn}\\n\"\n",
    "                f\"    Acc {acc:.4f} | ROC-AUC {roc:.4f} | AUC-PR {aucpr:.4f}\\n\"\n",
    "            )\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Split\":           [1, 2, 3],\n",
    "        \"OptimizationTime\": optimization_times,\n",
    "        \"TrainingTime\":     training_times,\n",
    "        \"InferenceTime\":    inference_times,\n",
    "        \"Accuracy\":         v_accuracy_test,\n",
    "        \"Specificity\":      v_specificity,\n",
    "        \"Precision\":        v_precision,\n",
    "        \"Recall\":           v_recall,\n",
    "        \"F1Score\":          v_f1score,\n",
    "        \"ROC_AUC\":          v_roc,\n",
    "        \"AUC_PR\":           v_aucpr\n",
    "    })\n",
    "    os.makedirs('./Results_Transformer_optuna', exist_ok=True)\n",
    "    summary_df.to_csv('./Results_Transformer_optuna/summary_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d440b",
   "metadata": {},
   "source": [
    "## RESULTS (PERFORMANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb74a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 62.26 +- 6.72\n",
      "Specificity: 84.96 +- 6.33\n",
      "Precision: 44.67 +- 7.91\n",
      "F1-score: 51.06 +- 3.44\n",
      "ROC-AUC: 78.28 +- 2.01\n",
      "AUC-PR: 49.30 +- 3.70\n",
      "Test Accuracy: 81.52 +- 4.38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = './Results_Transformer_optuna'\n",
    "summary_path = os.path.join(directory, \"summary_metrics.csv\")\n",
    "summary_df = pd.read_csv(summary_path)\n",
    "\n",
    "\n",
    "def calculateKPI(parameter):\n",
    "    \"\"\"\n",
    "    This function calculate the mean and deviation of a set of values of\n",
    "    a given performance indicator.\n",
    "    \n",
    "    Returns: Mean and std (float)\n",
    "    \"\"\"\n",
    "    mean = round(np.mean(parameter)*100, 2)\n",
    "    deviation = round(np.sqrt(np.sum(np.power(parameter - np.mean(parameter), 2) / len(parameter)))*100, 2)\n",
    "    return mean, deviation\n",
    "\n",
    "def format_metric_line(metric_name, mean_value, deviation_value):\n",
    "    return f\"{metric_name}: {mean_value:.2f} +- {deviation_value:.2f}\\n\"\n",
    "\n",
    "mean_test, deviation_test = calculateKPI(summary_df[\"Accuracy\"])\n",
    "mean_specificity, deviation_specificity = calculateKPI(summary_df[\"Specificity\"])\n",
    "mean_recall, deviation_recall = calculateKPI(summary_df[\"Recall\"])\n",
    "mean_f1, deviation_f1 = calculateKPI(summary_df[\"F1Score\"])\n",
    "mean_precision, deviation_precision = calculateKPI(summary_df[\"Precision\"])\n",
    "mean_roc, deviation_roc = calculateKPI(summary_df[\"ROC_AUC\"])\n",
    "mean_aucpr, deviation_aucpr = calculateKPI(summary_df[\"AUC_PR\"])  \n",
    "\n",
    "results = \"\"\n",
    "results += format_metric_line(\"Test Accuracy\", mean_test, deviation_test)\n",
    "results += format_metric_line(\"Specificity\", mean_specificity, deviation_specificity)\n",
    "results += format_metric_line(\"Sensitivity\", mean_recall, deviation_recall)\n",
    "results += format_metric_line(\"Precision\", mean_precision, deviation_precision)\n",
    "results += format_metric_line(\"F1-score\", mean_f1, deviation_f1)\n",
    "results += format_metric_line(\"ROC-AUC\", mean_roc, deviation_roc)\n",
    "results += format_metric_line(\"AUC-PR\", mean_aucpr, deviation_aucpr) \n",
    "\n",
    "final_results = (\n",
    "    f\"Sensitivity: {mean_recall:.2f} +- {deviation_recall:.2f}\\n\"\n",
    "    f\"Specificity: {mean_specificity:.2f} +- {deviation_specificity:.2f}\\n\"\n",
    "    f\"Precision: {mean_precision:.2f} +- {deviation_precision:.2f}\\n\"\n",
    "    f\"F1-score: {mean_f1:.2f} +- {deviation_f1:.2f}\\n\"\n",
    "    f\"ROC-AUC: {mean_roc:.2f} +- {deviation_roc:.2f}\\n\"\n",
    "    f\"AUC-PR: {mean_aucpr:.2f} +- {deviation_aucpr:.2f}\\n\" \n",
    "    f\"Test Accuracy: {mean_test:.2f} +- {deviation_test:.2f}\\n\"\n",
    ")\n",
    "\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240e050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
